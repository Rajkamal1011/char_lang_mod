{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86204dfd-46fb-4568-bbf1-71ee997349a7",
   "metadata": {},
   "source": [
    "### Dataset Loading or Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb10e842-b868-40a2-8d99-5bbd81fca281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rajkamal', 'Rajendra', 'Kamal', 'Chetan', 'Rohinee', 'Ansh', 'Dipak', 'Pratik', 'Arnav', 'Komal']\n",
      "My name is anthongy gonzalvez, mai duniya mai akela hu!\n"
     ]
    }
   ],
   "source": [
    "#Toy Dataset of names\n",
    "names = [\"Rajkamal\", \"Rajendra\", \"Kamal\", \"Chetan\", \"Rohinee\",\"Ansh\", \"Dipak\", \"Pratik\", \"Arnav\", \"Komal\"]\n",
    "print(names)\n",
    "#Toy Dataset of alien language\n",
    "alien_language = \"My name is anthongy gonzalvez, mai duniya mai akela hu!\"\n",
    "print(alien_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2117b255-14ab-4f39-9bf7-afa2447d885a",
   "metadata": {},
   "source": [
    "### Vocabulary + ctoi + itoc Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d84da7aa-766f-4c7b-98f5-70a86cb0599c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab of names:  ['a', 'c', 'd', 'e', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'v']\n",
      "ctoi of names:  {'a': 1, 'c': 2, 'd': 3, 'e': 4, 'h': 5, 'i': 6, 'j': 7, 'k': 8, 'l': 9, 'm': 10, 'n': 11, 'o': 12, 'p': 13, 'r': 14, 's': 15, 't': 16, 'v': 17, '$': 0}\n",
      "itos of names:  {1: 'a', 2: 'c', 3: 'd', 4: 'e', 5: 'h', 6: 'i', 7: 'j', 8: 'k', 9: 'l', 10: 'm', 11: 'n', 12: 'o', 13: 'p', 14: 'r', 15: 's', 16: 't', 17: 'v', 0: '$'}\n",
      "Vocab of alien language:  [' ', '!', ',', 'a', 'd', 'e', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 's', 't', 'u', 'v', 'y', 'z']\n",
      "ctoi of alien language {' ': 1, '!': 2, ',': 3, 'a': 4, 'd': 5, 'e': 6, 'g': 7, 'h': 8, 'i': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 's': 15, 't': 16, 'u': 17, 'v': 18, 'y': 19, 'z': 20, '$': 0}\n",
      "itoc of alien language {1: ' ', 2: '!', 3: ',', 4: 'a', 5: 'd', 6: 'e', 7: 'g', 8: 'h', 9: 'i', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 's', 16: 't', 17: 'u', 18: 'v', 19: 'y', 20: 'z', 0: '$'}\n"
     ]
    }
   ],
   "source": [
    "names = [name.lower() for name in names]\n",
    "vocab_names = sorted(list(set(''.join(names))))\n",
    "print(\"Vocab of names: \",vocab_names)\n",
    "#I am taking start and end symbol to be '$' - it will serve as input for first character as output, so it is necessary ath the start\n",
    "#and at the end also it is necessary because we want to signify end of the name\n",
    "ctoi_names = {}\n",
    "ctoi_names = {ch:idx+1 for idx,ch in enumerate(vocab_names)}#gives us token ids and helps in encoding\n",
    "ctoi_names['$'] = 0\n",
    "print(\"ctoi of names: \",ctoi_names)\n",
    "itoc_names = {}\n",
    "itoc_names = {idx+1:ch for idx,ch in enumerate(vocab_names)}#gives us token ids and helps in encoding\n",
    "itoc_names[0] = '$'\n",
    "print(\"itos of names: \", itoc_names)\n",
    "\n",
    "#We will assume the alien language to have \n",
    "alien_language = alien_language.lower()\n",
    "vocab_alien_lang = sorted(list(set(alien_language)))\n",
    "print(\"Vocab of alien language: \", vocab_alien_lang)\n",
    "ctoi_alien_lang = {}\n",
    "ctoi_alien_lang = {ch:idx+1 for idx,ch in enumerate(vocab_alien_lang)}#gives us token ids and helps in encoding\n",
    "ctoi_alien_lang['$'] = 0\n",
    "print(\"ctoi of alien language\", ctoi_alien_lang)\n",
    "itoc_alien_lang = {idx+1:ch for idx,ch in enumerate(vocab_alien_lang)}#gives us dekoded tokens from token ids and hence, helps in decoding\n",
    "itoc_alien_lang[0] = '$'\n",
    "print(\"itoc of alien language\", itoc_alien_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787d2b0-a265-4683-93f5-455ca0cacdf9",
   "metadata": {},
   "source": [
    "### Simple Bigram Character-level language model\n",
    "* Get counts matrix/tensor with dimension vocab*vocab\n",
    "* Convert it into Probability matrix/tensor by dividing by row sum to each row element - row-wise\n",
    "* Use this Probability matrix/tensor for prediction and stuff(computing the negative log likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "189fa2a9-404d-4ef1-a173-2e6ef847832d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 18])\n",
      "tensor([[0., 2., 1., 1., 0., 0., 0., 0., 2., 0., 0., 0., 0., 1., 3., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 2., 1., 3., 2., 2., 0., 0., 1., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.]])\n",
      "torch.Size([18, 18])\n"
     ]
    }
   ],
   "source": [
    "#Build Bigram Character-level language model for names\n",
    "\n",
    "N = torch.zeros((len(vocab_names)+1 , len(vocab_names)+1))\n",
    "print(N.shape)\n",
    "\n",
    "for w in names:\n",
    "    char_list_w = ['$'] + list(w) + ['$']\n",
    "    for ch1, ch2 in zip(char_list_w, char_list_w[1:]):\n",
    "        ix1 = ctoi_names[ch1]\n",
    "        ix2 = ctoi_names[ch2]\n",
    "        N[ix1, ix2] += 1\n",
    "\n",
    "print(N[:5])\n",
    "print(N.shape)\n",
    "\n",
    "P = N+1\n",
    "P /= P.sum(dim=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c6f134d-04f0-4b92-9ba6-daab753f26b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ddjijajrnnhvdjejasrndimtrpiatmvcrrtlhsikhnasljcsekntojavvmmhictraiekvlolsamsh$\n",
      "edcvptesthjrslhvrdh$\n",
      "tppcmmapanar$\n",
      "$\n",
      "dskatjk$\n"
     ]
    }
   ],
   "source": [
    "#Sampling using the distributions captured by P\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        p = P[ix]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
    "        out.append(itoc_names[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aa932fd8-0035-44dc-9424-7d89d79dbf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood:  tensor(-154.4681)\n",
      "nll:  tensor(154.4681)\n",
      "Average negative log likelihood:  tensor(2.2387)\n"
     ]
    }
   ],
   "source": [
    "#Computing the goodness of this model based on P - using the log likelihood of the predictions\n",
    "\n",
    "log_likelihood = 0.0\n",
    "n = 0 #Total samples, so that at last we can get average of log likelihood\n",
    "\n",
    "for w in names:\n",
    "    char_list_w = ['$'] + list(w) + ['$']\n",
    "    for ch1, ch2 in zip(char_list_w, char_list_w[1:]):\n",
    "        ix1 = ctoi_names[ch1]\n",
    "        ix2 = ctoi_names[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "\n",
    "print(\"Log likelihood: \", log_likelihood)\n",
    "nll = -log_likelihood\n",
    "print(\"nll: \",nll)\n",
    "print(\"Average negative log likelihood: \", nll/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7dccbff5-7bbe-46ae-8b5a-639e2dfb9102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 21])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build Simple Bigram character-level language model\n",
    "\n",
    "N_ = torch.zeros(((len(vocab_alien_lang)+1, len(vocab_alien_lang)+1)))\n",
    "\n",
    "block_size = 1\n",
    "\n",
    "context = [0] * block_size\n",
    "char_word_list = ['$'] + list(alien_language) + ['$']\n",
    "for ch1, ch2 in zip(char_word_list, char_word_list[1:]):\n",
    "    ix1 = ctoi_alien_lang[ch1]\n",
    "    ix2 = ctoi_alien_lang[ch2]\n",
    "    N_[ix1,ix2] += 1\n",
    "\n",
    "N_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02962dde-97c7-4145-ba77-5815c03b3ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iegtg!hhh,dgadstnuun,mzz$\n",
      "vgukd$\n",
      "m  tlv,!sulyoozklnnv$\n",
      "siyheiuy!!vluvhdv  d$\n",
      "ineoetnels!$\n"
     ]
    }
   ],
   "source": [
    "#Sampling from this model\n",
    "\n",
    "P_ = N_+1\n",
    "P_ /= P_/P_.sum(dim=1, keepdims=True)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        p = P_[ix]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
    "        out.append(itoc_alien_lang[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b44ab8a-21f6-4af1-bcae-cf808687bce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(181.0132)\n",
      "NLL:  tensor(-181.0132)\n",
      "Average negative loss likelihood:  tensor(-3.2324)\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "char_list_alien_lang = ['$'] + list(alien_language) + ['$']\n",
    "for ch1, ch2 in zip(char_list_alien_lang, char_list_alien_lang[1:]):\n",
    "    ix1 = ctoi_alien_lang[ch1]\n",
    "    ix2 = ctoi_alien_lang[ch2]\n",
    "    prob = P_[ix1, ix2]\n",
    "    log_likelihood += torch.log(prob)\n",
    "    n += 1\n",
    "\n",
    "print(log_likelihood)\n",
    "nll = -log_likelihood\n",
    "print(\"NLL: \",nll)\n",
    "print(\"Average negative loss likelihood: \", nll/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e07a5-6aaa-41d3-a489-8c10b5af60e7",
   "metadata": {},
   "source": [
    "Note: Dataset building means *having input and output component - in case of n-gram character-level lanugage models it is going to be (n-1) characters as input and nth character as output - this window will keep sliding till we reach the text's end*.\n",
    "As, simple bigram language model don't have any concept of \"training\" as such since, it is simply based on the statistical modelling achieved by computing N and then, P - which is thenused as the beacon of character-level prediction. Hence, it doesn't need dataset creation, computaiton of N and P is enough. Above, we also have show how to use it for sampling as well as computed the log likelihood of all the samples here (of the gold distribution captured directly from the dataset.)\n",
    "\n",
    "Now, we will implement same bigram model but with trainable parameter (W) which will start from random initializaiton and then learn to become P or better than P - to give us equivalent or even smaller negative average log likelihood! But, before that, let's build the training dataset for it!\n",
    "\n",
    "### Dataset Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae7f2ff5-fc01-4f62-bf08-31db2fe6896b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57, 1]) torch.Size([57])\n",
      "torch.Size([6, 1]) torch.Size([6])\n",
      "torch.Size([6, 1]) torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "#Building dataset for names - with respect to bigram language model\n",
    "\n",
    "import torch\n",
    "\n",
    "block_size = 1 #context length: how many characters do we take to predict the next one\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size #Give context as $ character's token_id\n",
    "        for ch in w + '$':\n",
    "            ix = ctoi_names[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "        \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X,Y\n",
    "\n",
    "#import random\n",
    "#random.seed(42)\n",
    "\n",
    "n1 = int(0.8 * len(names))\n",
    "n2 = int(0.9 * len(names))\n",
    "\n",
    "Xtr_names, Ytr_names = build_dataset(names[:n1])\n",
    "Xdev_names, Ydev_names = build_dataset(names[n1:n2])\n",
    "Xte_names, Yte_names = build_dataset(names[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abfd193d-03cd-4c2d-b72e-78e98f192544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0],\n",
       "         [14],\n",
       "         [ 1],\n",
       "         [ 7],\n",
       "         [ 8]]),\n",
       " tensor([14,  1,  7,  8,  1]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr_names[:5], Ytr_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c85e59d-ac4c-414b-b0d1-f3a092b17717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(''.join(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbd5a2cf-74ec-40fb-914b-74150c24fe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45, 1]) torch.Size([45])\n",
      "torch.Size([6, 1]) torch.Size([6])\n",
      "torch.Size([7, 1]) torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "#Building dataset for alien language with respect to bigram language model\n",
    "\n",
    "block_size = 1\n",
    "\n",
    "def build_dataset2(text):\n",
    "    X, Y = [], []\n",
    "    context = [0] * block_size\n",
    "    for ch in text + '$':\n",
    "        ix = ctoi_alien_lang[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X,Y\n",
    "\n",
    "n1 = int(0.8 * len(alien_language))\n",
    "n2 = int(0.9 * len(alien_language))\n",
    "\n",
    "Xtr_alien_lang, Ytr_alien_lang = build_dataset2(alien_language[:n1])\n",
    "Xdev_alien_lang, Ydev_alien_lang = build_dataset2(alien_language[n1:n2])\n",
    "Xte_alien_lang, Yte_alien_lang = build_dataset2(alien_language[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a6202a1-c5b0-4611-82ef-c810fd52dba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alien_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e946dc1-e5e5-4997-bdc9-4e570a976f44",
   "metadata": {},
   "source": [
    "### Using Neural Network to Learn Distribution of Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bf1bbeee-a516-4ad0-ae11-4353304f46d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_names = torch.randn((len(vocab_names)+1, len(vocab_names)+1), requires_grad = True) #requires_grad = True, since, it is a trainable parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0bb43709-bfdb-40cd-8c05-91b16ca8a4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  started!\n",
      "torch.Size([1, 18])\n",
      "tensor([[  0.4639,  19.7803,  14.3185,  25.7107,   0.4684,   0.4596,   0.3184,\n",
      "           0.4230,  12.4094,   0.3814,   0.4649,   0.3559,   0.4005, 185.6587,\n",
      "          18.1433,   0.4399,   0.4318,   0.4260]], grad_fn=<ExpBackward0>)\n",
      "2.771587371826172\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "import torch.nn as nn\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(\"Epoch: \", i, \" started!\")\n",
    "    for j in range(Xtr.shape[0]):\n",
    "        #forward pass\n",
    "        #print(Xtr[j])\n",
    "        #print(Xtr[j].nelement())\n",
    "        xenc = nn.functional.one_hot(Xtr[j], num_classes=len(vocab_names)+1).float()\n",
    "        logits = xenc @ W_names\n",
    "        print(logits.shape)\n",
    "        counts = logits.exp()\n",
    "        print(counts)\n",
    "        probs = counts/counts.sum(dim=1, keepdims=True)\n",
    "        #print(probs.sum(dim=1, keepdims=True))\n",
    "        #print(probs.shape)\n",
    "        #print(probs)\n",
    "        loss = -probs[torch.arange(Xtr[j].nelement()), Ytr[j]].log().mean() + 0.01*(W_names**2).mean()\n",
    "\n",
    "        #print(loss.item())\n",
    "\n",
    "        #backward pass\n",
    "        W_names.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        #update\n",
    "        W_names.data += -3 * W_names.grad\n",
    "        break\n",
    "    print(loss.item())\n",
    "    break\n",
    "        #break\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6be43cca-13ad-4c8f-b2ee-3ca2acb8abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_alien_lang = torch.randn((len(vocab_alien_lang)+1, len(vocab_alien_lang)+1), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5965b1d5-60f8-42fb-8977-6afda3cbd072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  started!\n",
      "0.4019641876220703\n",
      "Epoch  1  started!\n",
      "0.19466714560985565\n",
      "Epoch  2  started!\n",
      "0.15605615079402924\n",
      "Epoch  3  started!\n",
      "0.1395486295223236\n",
      "Epoch  4  started!\n",
      "0.13049766421318054\n",
      "Epoch  5  started!\n",
      "0.12486027926206589\n",
      "Epoch  6  started!\n",
      "0.12106199562549591\n",
      "Epoch  7  started!\n",
      "0.11835967004299164\n",
      "Epoch  8  started!\n",
      "0.11635943502187729\n",
      "Epoch  9  started!\n",
      "0.11483220756053925\n",
      "Epoch  10  started!\n",
      "0.11363725364208221\n",
      "Epoch  11  started!\n",
      "0.11268284171819687\n",
      "Epoch  12  started!\n",
      "0.11190763115882874\n",
      "Epoch  13  started!\n",
      "0.11126876622438431\n",
      "Epoch  14  started!\n",
      "0.11073500663042068\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "for i in range(epochs):\n",
    "    print(\"Epoch \",i,\" started!\")\n",
    "    for j in range(Xtr.shape[0]):\n",
    "        #forward pass\n",
    "        xenc = nn.functional.one_hot(Xtr[j], num_classes=len(vocab_alien_lang)+1).float()\n",
    "        logits = xenc @ W_alien_lang\n",
    "        #print(logits.shape)\n",
    "        counts = logits.exp()\n",
    "        #print(counts.shape)\n",
    "        probs = counts/counts.sum(dim=1, keepdims=True)\n",
    "        loss = -probs[torch.arange(Xtr[j].nelement()), Ytr[j]].log().mean() + 0.01 * (W_alien_lang**2).mean()\n",
    "        #backward pass\n",
    "        W_alien_lang.grad = None\n",
    "        loss.backward()\n",
    "        #Update W\n",
    "        W_alien_lang.data += -3 * W_alien_lang.grad\n",
    "        #break\n",
    "    print(loss.item())\n",
    "    #break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7f7c9-9458-426c-8b20-00921df547dd",
   "metadata": {},
   "source": [
    "### Sampling from neural network based bigram character-level language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "62b4bc00-b550-491d-9075-1c801ab0d7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratikal$\n",
      "ratik$\n",
      "ratik$\n",
      "ransh$\n",
      "dik$\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        #forward pass\n",
    "        xenc = nn.functional.one_hot(torch.tensor([ix]), num_classes = len(vocab_names)+1).float()\n",
    "        logits = xenc @ W_names\n",
    "        counts = logits.exp()\n",
    "        #print(counts.shape)\n",
    "        probs = counts/counts.sum(dim=1, keepdims=True)\n",
    "\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True).item()\n",
    "        out.append(itoc_names[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dea15b6e-c8d6-4680-88e4-89247ad8242a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lsd$\n",
      "!d$\n",
      ",eh$\n",
      " teno $\n",
      "no tlsd$\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        xenc = nn.functional.one_hot(torch.tensor([ix]), num_classes = len(vocab_alien_lang)+1).float()\n",
    "        logits = xenc @ W_alien_lang\n",
    "        counts = logits.exp()\n",
    "        probs = counts/counts.sum(dim=1, keepdim=True)\n",
    "\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True).item()\n",
    "        out.append(itoc_alien_lang[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8a3e79-00c1-482b-9355-cc187a51c4fa",
   "metadata": {},
   "source": [
    "Currently, learning weight matrix seems simple, since it is just vocab_size * vocab_size. This is because block_size = 1.<br>\n",
    "If we were to increase block_size to 2, then, (vocab_size * vocab_size) - all possible pairs of two character context * vocab_size(distribution of probs over vocabulary), so weight matrix's size will be (vocab_size**2, vocab_size)!<br>\n",
    "So, as the block_size keeps increasing number of parameters are going to exponentially increase! Similarly, for the simple n-gram language model also, the size of N and P will exponentially grow to capture all context nuances - they will mostly be sparse!\n",
    "\n",
    "So, this is a good solution for bigram or say at max trigram models, but for general n-gram models this become infeasible, this is where Bengio et al's MLP comes to our rescue!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d445f237-f78f-4f10-89f8-054ff378b016",
   "metadata": {},
   "source": [
    "### MLP for character-level n-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0b3f3cc2-e55a-4e34-89df-742d73b907a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3098"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 10 #embedding dimensions\n",
    "hidden_dim = 100\n",
    "block_size = 1\n",
    "C = torch.randn((len(vocab_names)+1, emb_dim), requires_grad = True)\n",
    "W1 = torch.randn((emb_dim*block_size, hidden_dim), requires_grad = True)\n",
    "b1 = torch.randn(hidden_dim, requires_grad = True)\n",
    "W2 = torch.randn((hidden_dim, len(vocab_names)+1), requires_grad = True)\n",
    "b2 = torch.randn(len(vocab_names)+1, requires_grad = True)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0b1af429-0b88-4332-8977-eb4c5c119b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  started!\n",
      "27.830625534057617\n",
      "Epoch  1  started!\n",
      "26.544397354125977\n",
      "Epoch  2  started!\n",
      "25.539392471313477\n",
      "Epoch  3  started!\n",
      "24.680999755859375\n",
      "Epoch  4  started!\n",
      "23.83167266845703\n",
      "Epoch  5  started!\n",
      "22.983932495117188\n",
      "Epoch  6  started!\n",
      "22.155757904052734\n",
      "Epoch  7  started!\n",
      "21.356372833251953\n",
      "Epoch  8  started!\n",
      "20.591529846191406\n",
      "Epoch  9  started!\n",
      "19.865468978881836\n",
      "Epoch  10  started!\n",
      "19.179353713989258\n",
      "Epoch  11  started!\n",
      "18.533411026000977\n",
      "Epoch  12  started!\n",
      "17.92671775817871\n",
      "Epoch  13  started!\n",
      "17.356670379638672\n",
      "Epoch  14  started!\n",
      "16.819229125976562\n",
      "Epoch  15  started!\n",
      "16.308870315551758\n",
      "Epoch  16  started!\n",
      "15.819832801818848\n",
      "Epoch  17  started!\n",
      "15.34683895111084\n",
      "Epoch  18  started!\n",
      "14.885547637939453\n",
      "Epoch  19  started!\n",
      "14.432499885559082\n",
      "Epoch  20  started!\n",
      "13.984454154968262\n",
      "Epoch  21  started!\n",
      "13.537090301513672\n",
      "Epoch  22  started!\n",
      "13.084402084350586\n",
      "Epoch  23  started!\n",
      "12.62091064453125\n",
      "Epoch  24  started!\n",
      "12.145729064941406\n",
      "Epoch  25  started!\n",
      "11.662263870239258\n",
      "Epoch  26  started!\n",
      "11.174885749816895\n",
      "Epoch  27  started!\n",
      "10.687198638916016\n",
      "Epoch  28  started!\n",
      "10.201745986938477\n",
      "Epoch  29  started!\n",
      "9.720243453979492\n",
      "Epoch  30  started!\n",
      "9.243844985961914\n",
      "Epoch  31  started!\n",
      "8.773436546325684\n",
      "Epoch  32  started!\n",
      "8.309630393981934\n",
      "Epoch  33  started!\n",
      "7.8528265953063965\n",
      "Epoch  34  started!\n",
      "7.403139591217041\n",
      "Epoch  35  started!\n",
      "6.960361003875732\n",
      "Epoch  36  started!\n",
      "6.524133682250977\n",
      "Epoch  37  started!\n",
      "6.094015598297119\n",
      "Epoch  38  started!\n",
      "5.669582366943359\n",
      "Epoch  39  started!\n",
      "5.2506232261657715\n",
      "Epoch  40  started!\n",
      "4.837311744689941\n",
      "Epoch  41  started!\n",
      "4.430527687072754\n",
      "Epoch  42  started!\n",
      "4.032076835632324\n",
      "Epoch  43  started!\n",
      "3.64457368850708\n",
      "Epoch  44  started!\n",
      "3.271556854248047\n",
      "Epoch  45  started!\n",
      "2.9173502922058105\n",
      "Epoch  46  started!\n",
      "2.586963653564453\n",
      "Epoch  47  started!\n",
      "2.2855422496795654\n",
      "Epoch  48  started!\n",
      "2.017439365386963\n",
      "Epoch  49  started!\n",
      "1.7853631973266602\n",
      "Epoch  50  started!\n",
      "1.5896735191345215\n",
      "Epoch  51  started!\n",
      "1.4284272193908691\n",
      "Epoch  52  started!\n",
      "1.2979201078414917\n",
      "Epoch  53  started!\n",
      "1.1936039924621582\n",
      "Epoch  54  started!\n",
      "1.1107765436172485\n",
      "Epoch  55  started!\n",
      "1.0451996326446533\n",
      "Epoch  56  started!\n",
      "0.9931853413581848\n",
      "Epoch  57  started!\n",
      "0.9517630934715271\n",
      "Epoch  58  started!\n",
      "0.9185924530029297\n",
      "Epoch  59  started!\n",
      "0.8918070197105408\n",
      "Epoch  60  started!\n",
      "0.8699883222579956\n",
      "Epoch  61  started!\n",
      "0.8520437479019165\n",
      "Epoch  62  started!\n",
      "0.8371392488479614\n",
      "Epoch  63  started!\n",
      "0.8246259689331055\n",
      "Epoch  64  started!\n",
      "0.814009428024292\n",
      "Epoch  65  started!\n",
      "0.8049140572547913\n",
      "Epoch  66  started!\n",
      "0.7970421314239502\n",
      "Epoch  67  started!\n",
      "0.7901508212089539\n",
      "Epoch  68  started!\n",
      "0.7840813994407654\n",
      "Epoch  69  started!\n",
      "0.7786787748336792\n",
      "Epoch  70  started!\n",
      "0.7738420367240906\n",
      "Epoch  71  started!\n",
      "0.7694730758666992\n",
      "Epoch  72  started!\n",
      "0.7655055522918701\n",
      "Epoch  73  started!\n",
      "0.7618797421455383\n",
      "Epoch  74  started!\n",
      "0.7585546970367432\n",
      "Epoch  75  started!\n",
      "0.7554943561553955\n",
      "Epoch  76  started!\n",
      "0.7526617646217346\n",
      "Epoch  77  started!\n",
      "0.7500337958335876\n",
      "Epoch  78  started!\n",
      "0.7475846409797668\n",
      "Epoch  79  started!\n",
      "0.7452966570854187\n",
      "Epoch  80  started!\n",
      "0.7431556582450867\n",
      "Epoch  81  started!\n",
      "0.7411529421806335\n",
      "Epoch  82  started!\n",
      "0.7392664551734924\n",
      "Epoch  83  started!\n",
      "0.7374966740608215\n",
      "Epoch  84  started!\n",
      "0.7358168363571167\n",
      "Epoch  85  started!\n",
      "0.7342336773872375\n",
      "Epoch  86  started!\n",
      "0.7327466607093811\n",
      "Epoch  87  started!\n",
      "0.731324315071106\n",
      "Epoch  88  started!\n",
      "0.7299785614013672\n",
      "Epoch  89  started!\n",
      "0.7286964654922485\n",
      "Epoch  90  started!\n",
      "0.7274813055992126\n",
      "Epoch  91  started!\n",
      "0.7263278961181641\n",
      "Epoch  92  started!\n",
      "0.7252235412597656\n",
      "Epoch  93  started!\n",
      "0.7241717576980591\n",
      "Epoch  94  started!\n",
      "0.7231671810150146\n",
      "Epoch  95  started!\n",
      "0.7222031950950623\n",
      "Epoch  96  started!\n",
      "0.721281111240387\n",
      "Epoch  97  started!\n",
      "0.7204018831253052\n",
      "Epoch  98  started!\n",
      "0.7195550203323364\n",
      "Epoch  99  started!\n",
      "0.7187464237213135\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for i in range(epochs):\n",
    "    print(\"Epoch \",i,\" started!\")\n",
    "    for j in range(Xtr.shape[0]):\n",
    "        emb = C[Xtr[j]]\n",
    "        #print(emb.shape)\n",
    "        h = torch.tanh(emb.view(-1, emb_dim*block_size) @ W1 + b1)\n",
    "        #print(h.shape)\n",
    "        logits = h @ W2 + b2\n",
    "        #print(logits.shape)\n",
    "        #counts = logits.exp()\n",
    "        #probs = counts/counts.sum(1, keepdims=True)\n",
    "        #loss = torch.tensor(torch.arange(Xtr[j]), Ytr[j])\n",
    "        #print(logits.shape)\n",
    "        #print(Ytr_names[j])\n",
    "        #print(Ytr_names.shape)\n",
    "        Ytr_names_one_hot = nn.functional.one_hot(Ytr_names[j], num_classes=len(vocab_names)+1).float()\n",
    "        #print(Ytr_names_one_hot.shape)\n",
    "        loss = nn.functional.cross_entropy(logits, Ytr_names_one_hot.view(-1, len(vocab_names)+1))\n",
    "\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        for p in parameters:\n",
    "            p.data += -0.001 * p.grad\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28688607-9760-42eb-8739-7409add05213",
   "metadata": {},
   "source": [
    "I am propagating gradients with respect to loss of single sample rather than a batch or that of the whole samples, that's why such unstable training is seen in loss!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "50e8469a-bada-4b01-986d-cff0e47e49fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3431"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 10\n",
    "block_size = 1\n",
    "C_ = torch.randn((len(vocab_alien_lang)+1, emb_dim), requires_grad = True)\n",
    "W1_ = torch.randn((emb_dim * block_size, hidden_dim), requires_grad = True)\n",
    "b1_ = torch.randn(hidden_dim, requires_grad = True)\n",
    "W2_ = torch.randn((hidden_dim, len(vocab_alien_lang)+1), requires_grad = True)\n",
    "b2_ = torch.randn(len(vocab_alien_lang)+1, requires_grad = True)\n",
    "\n",
    "parameters = [C_, W1_, b1_, W2_, b2_]\n",
    "\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3f1c53a0-6005-45cb-8c65-16e64537a8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  has started\n",
      "torch.Size([1, 18])\n",
      "torch.Size([21])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(Ytr_names_one_hot\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 13\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYtr_names_one_hot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab_alien_lang\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[1;32m     16\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(\"Epoch \",i, \" has started\")\n",
    "    for j in range(Xtr.shape[0]):\n",
    "        emb = C_[Xtr[j]]\n",
    "        h = torch.tanh(emb.view(-1, emb_dim * block_size) @ W1_ + b1_)\n",
    "        logits = h @ W2_ + b2_\n",
    "\n",
    "        Ytr_names_one_hot = nn.functional.one_hot(Ytr[j], num_classes=len(vocab_alien_lang)+1).float()\n",
    "        print(logits.shape)\n",
    "        print(Ytr_names_one_hot.shape)\n",
    "        loss = nn.functional.cross_entropy(logits, Ytr_names_one_hot.view(-1, len(vocab_alien_lang)+1))\n",
    "\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        for p in parameters:\n",
    "            p.data += -0.01 * p.grad\n",
    "    print(loss.item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f448e-dc8c-4737-8799-26bb7e69996a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
